{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a87c604",
   "metadata": {},
   "source": [
    "# Semi-supervised approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebd00b",
   "metadata": {},
   "source": [
    "\n",
    "We use **semi-supervised learning**: training a regressor with a small labeled set plus extra unlabeled data to improve generalization. Typical families include:\n",
    "- **Graph-based** methods (label propagation, Laplacian-regularized regression).\n",
    "- **Consistency regularization** (teacher–student / Mean-Teacher, perturbation invariance).\n",
    "- **Co-/Tri-training** (multiple views/models teach each other).\n",
    "- **Generative** approaches (e.g., VAE-based feature learning + supervised head).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afec7c2",
   "metadata": {},
   "source": [
    "\n",
    "## The approach we use\n",
    "We will use the approach called **self-training**.\n",
    "We fit a **RandomForestRegressor** on **labeled train** (baseline), build the **unlabeled pool from train**, and iterate. \n",
    "At each round we predict on the pool, compute per-tree std **$\\sigma$** as an uncertainty proxy, keep the **lowest- $\\sigma$ quantile (e.g., 20%)**, add those samples with **uniform weight = 0.5** (true labels keep weight = 1.0), refit with sample weights, and report RMSE/MAE/R² on the **labeled test**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eeb452",
   "metadata": {},
   "source": [
    "\n",
    "## Pseudocode\n",
    "```pseudo\n",
    "for i in range(nb_iterations):\n",
    "  if U_train empty: break\n",
    "  μ, σ = per-tree mean/std predictions on U_train\n",
    "  S = indices with σ in lowest quantile (e.g., 20%)\n",
    "  add (U_train[S], label=μ[S], weight=0.5) to training set\n",
    "  remove S from U_train\n",
    "  refit RandomForest with sample_weight; report metrics on labeled test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a8441",
   "metadata": {},
   "source": [
    "# Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4ba8fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5412f13",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77611348",
   "metadata": {},
   "source": [
    "    Note: In this part we will directly prevent the leakage by dropping the post-process/mechanical fields:\n",
    "    `ultimate_tensile_strength_mpa`, `elongation_percent`, `reduction_of_area_percent`, `charpy_temperature_c`, `charpy_impact_toughness_j`, `haz_hardness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "305fbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load\n",
    "df_train = pd.read_csv(\"preprocess_data/train_processed.csv\")\n",
    "df_test  = pd.read_csv(\"preprocess_data/test_processed.csv\")\n",
    "\n",
    "# drop leakage columns\n",
    "leak_cols = [\n",
    "    \"ultimate_tensile_strength_mpa\",\n",
    "    \"elongation_percent\",\n",
    "    \"reduction_of_area_percent\",\n",
    "    \"charpy_temperature_c\",\n",
    "    \"charpy_impact_toughness_j\",\n",
    "    \"haz_hardness\",\n",
    "]\n",
    "df_train = df_train.drop(columns=leak_cols)\n",
    "df_test  = df_test.drop(columns=leak_cols)\n",
    "\n",
    "# Split labeled / unlabeled\n",
    "y_col = \"yield_strength_mpa\"\n",
    "train_labeled      = df_train[df_train[y_col].notna()].copy()\n",
    "train_unlabeled_X  = df_train[df_train[y_col].isna()].drop(columns=[y_col]).copy()\n",
    "\n",
    "test_labeled       = df_test[df_test[y_col].notna()].copy()   # only for evaluation\n",
    "# (ignore test unlabeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "31a72e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'baseline', 'rmse': 44.94810315018374, 'mae': 29.07128949336176, 'r2': 0.7971385235889683}\n"
     ]
    }
   ],
   "source": [
    "# Consistent feature set\n",
    "feature_cols = [c for c in df_train.columns if c != y_col]\n",
    "X_train_L = train_labeled[feature_cols]\n",
    "y_train_L = train_labeled[y_col].values\n",
    "X_test_L  = test_labeled[feature_cols]\n",
    "y_test_L  = test_labeled[y_col].values\n",
    "\n",
    "#  Baseline: train on train labeled only\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=400, max_depth=None, min_samples_split = 3,\n",
    "    max_features=0.5, random_state=42, n_jobs=-1, bootstrap=True\n",
    ")\n",
    "model.fit(X_train_L, y_train_L)\n",
    "\n",
    "pred_base = model.predict(X_test_L)\n",
    "print({\n",
    "    \"stage\": \"baseline\",\n",
    "    \"rmse\": float(np.sqrt(mean_squared_error(y_test_L, pred_base))),\n",
    "    \"mae\":  float(mean_absolute_error(y_test_L, pred_base)),\n",
    "    \"r2\":   float(r2_score(y_test_L, pred_base))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c49c1d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: labeled=613, unlabeled=708\n",
      "{'iter': 1, 'rmse': 44.84354299652713, 'mae': 29.444082898577804, 'r2': 0.7980812354917506, 'added_total': 142}\n",
      "Iteration 2: labeled=755, unlabeled=566\n",
      "{'iter': 2, 'rmse': 45.06221936435447, 'mae': 29.595129506424954, 'r2': 0.7961071487852592, 'added_total': 259}\n",
      "Iteration 3: labeled=872, unlabeled=449\n",
      "{'iter': 3, 'rmse': 44.35792843986998, 'mae': 29.329649912802267, 'r2': 0.802430747750497, 'added_total': 352}\n",
      "Iteration 4: labeled=965, unlabeled=356\n",
      "{'iter': 4, 'rmse': 45.14678570517127, 'mae': 29.770361786535197, 'r2': 0.7953411567105333, 'added_total': 424}\n",
      "Iteration 5: labeled=1037, unlabeled=284\n",
      "{'iter': 5, 'rmse': 44.694882703359475, 'mae': 29.693964047412898, 'r2': 0.7994177734066739, 'added_total': 481}\n"
     ]
    }
   ],
   "source": [
    "# Self-training: train unlabeled only, with uniform pseudo-label weight = 0.5\n",
    "X_unlabeled = train_unlabeled_X[feature_cols].copy()\n",
    "confidence_quantile = 0.20   # keep lowest 20% std each round\n",
    "iterations = 5\n",
    "\n",
    "X_lab = X_train_L.copy()\n",
    "y_lab = y_train_L.copy()\n",
    "sw_lab = np.ones(len(y_train_L), dtype=float)   # 1.0 for true labels\n",
    "\n",
    "for it in range(iterations):\n",
    "    if len(X_unlabeled) == 0:\n",
    "        break\n",
    "    print(f\"Iteration {it+1}: labeled={len(y_lab)}, unlabeled={len(X_unlabeled)}\")\n",
    "\n",
    "    # Tree-wise predictions -> mean and std per sample (uncertainty proxy)\n",
    "    X_unl_np = X_unlabeled.values  \n",
    "    tree_preds = np.stack([t.predict(X_unl_np) for t in model.estimators_], axis=1)\n",
    "    mu = tree_preds.mean(axis=1)\n",
    "    sigma = tree_preds.std(axis=1)\n",
    "\n",
    "    # Select most confident (lowest std)\n",
    "    thresh = np.quantile(sigma, confidence_quantile)\n",
    "    confident_idx = np.where(sigma <= thresh)[0]\n",
    "\n",
    "    # Uniform weights for pseudo-labels (all equal to 0.5)\n",
    "    w = np.full(len(confident_idx), 0.5)\n",
    "\n",
    "    # Add pseudo-labels + weights; remove from pool\n",
    "    X_lab = pd.concat([X_lab, X_unlabeled.iloc[confident_idx]], axis=0)\n",
    "    y_lab = np.concatenate([y_lab, mu[confident_idx]])\n",
    "    sw_lab = np.concatenate([sw_lab, w])\n",
    "    X_unlabeled = X_unlabeled.drop(index=X_unlabeled.index[confident_idx])\n",
    "\n",
    "    # Retrain RF with sample_weight\n",
    "    model = RandomForestRegressor(\n",
    "    n_estimators=400, max_depth=None, min_samples_split = 3,\n",
    "    max_features=0.5, random_state=42, n_jobs=-1, bootstrap=True)\n",
    "    model.fit(X_lab, y_lab, sample_weight=sw_lab)\n",
    "\n",
    "    # Quick eval on labeled test\n",
    "    y_hat = model.predict(X_test_L)\n",
    "    print({\n",
    "        \"iter\": it+1,\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_test_L, y_hat))),\n",
    "        \"mae\":  float(mean_absolute_error(y_test_L, y_hat)),\n",
    "        \"r2\":   float(r2_score(y_test_L, y_hat)),\n",
    "        \"added_total\": int(len(y_lab) - len(y_train_L))\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e9ef17fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'baseline', 'rmse': 44.94810315018374, 'mae': 29.07128949336176, 'r2': 0.7971385235889683}\n",
      "{'stage': 'final', 'rmse': 44.69488270335948, 'mae': 29.69396404741289, 'r2': 0.7994177734066739}\n"
     ]
    }
   ],
   "source": [
    "#print baseline to compare\n",
    "print({\n",
    "    \"stage\": \"baseline\",\n",
    "    \"rmse\": float(np.sqrt(mean_squared_error(y_test_L, pred_base))),\n",
    "    \"mae\":  float(mean_absolute_error(y_test_L, pred_base)),\n",
    "    \"r2\":   float(r2_score(y_test_L, pred_base))\n",
    "})\n",
    "# Final eval on labeled test\n",
    "y_pred = model.predict(X_test_L)\n",
    "print({\n",
    "    \"stage\": \"final\",\n",
    "    \"rmse\": float(np.sqrt(mean_squared_error(y_test_L, y_pred))),\n",
    "    \"mae\":  float(mean_absolute_error(y_test_L, y_pred)),\n",
    "    \"r2\":   float(r2_score(y_test_L, y_pred))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509212ac",
   "metadata": {},
   "source": [
    "## Comments \n",
    "Compared to the supervised baseline, the final self-trained model achieves RMSE 44.69 (≈ −0.56%), MAE 29.69 (≈ +2.1%), and $R^2$ 0.7994 (+0.0023).\n",
    "Verdict: the change is too small and inconsistent (RMSE down but MAE up) to be meaningful—very likely within random variability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
