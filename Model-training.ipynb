{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48551a2c",
   "metadata": {},
   "source": [
    "# Model training \n",
    "\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e3b45",
   "metadata": {},
   "source": [
    "This project aims to predict **weld quality** using different Machine Learning (ML) models based on process and material parameters.  \n",
    "\n",
    "The notebook includes: \n",
    "1. Introduction    \n",
    "2. Setup  \n",
    "3. Model definitions  \n",
    "4. Model training and cross-validation    \n",
    "5. Model comparison   \n",
    "6. Model interpretation  \n",
    "7. Final conclusions and recommendations for improving weld quality  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956313a2",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab04adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from utils_pre_processing import full_preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b625212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and preprocess data\n",
    "X_train, X_test, y_train, y_test, feature_names = full_preprocessing(\"data/welding_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7549a",
   "metadata": {},
   "source": [
    "## 3. Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22920310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    \"\"\"Return a dictionary of regression models to evaluate.\"\"\"\n",
    "   \n",
    "    models = {\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=6, subsample=0.8,\n",
    "            colsample_bytree=0.8, random_state=42, n_jobs=-1, verbosity=0\n",
    "        ),\n",
    "        'LightGBM': LGBMRegressor(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, verbose=-1\n",
    "        ),\n",
    "        'Random Forest': RandomForestRegressor(\n",
    "            n_estimators=500, max_depth=None, min_samples_split=5, min_samples_leaf=2,\n",
    "            max_features='sqrt', random_state=42, n_jobs=-1, bootstrap=True\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=5, subsample=0.8,\n",
    "            min_samples_split=5, min_samples_leaf=2, random_state=42\n",
    "        )\n",
    "    }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3e108",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3bc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a single model.\n",
    "\n",
    "    Returns:\n",
    "        dict: model performance metrics and timings\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    # Predictions\n",
    "    start = time.time()\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    pred_time = time.time() - start\n",
    "\n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_mape = np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100\n",
    "\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Test_MAPE': test_mape,\n",
    "        'CV_R2_mean': cv_scores.mean(),\n",
    "        'CV_R2_std': cv_scores.std(),\n",
    "        'Overfitting': train_r2 - test_r2,\n",
    "        'Train_time_sec': train_time,\n",
    "        'Predict_time_sec': pred_time,\n",
    "        'predictions_test': y_pred_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1dd65",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X_train, X_test, y_train, y_test, target_name='Target'):\n",
    "    \"\"\"\n",
    "    Train and compare multiple regression models.\n",
    "\n",
    "    Returns:\n",
    "        results_df: DataFrame with all metrics\n",
    "        predictions_dict: test predictions for each model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\\nModel Comparison - {target_name}\\n{'='*80}\")\n",
    "\n",
    "    models = get_models()\n",
    "    results, predictions = [], {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n Training {name}...\", end=\" \")\n",
    "        try:\n",
    "            res = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "            results.append(res)\n",
    "            predictions[name] = res['predictions_test']\n",
    "            print(f\"R² = {res['Test_R2']:.4f} | RMSE = {res['Test_RMSE']:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values('Test_R2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\nResults Summary (sorted by Test R²):\")\n",
    "    print(results_df[['Model', 'Test_R2', 'Test_RMSE', 'Test_MAE', 'CV_R2_mean', 'Overfitting']])\n",
    "\n",
    "    # Visualization 1: Test R² comparison\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(results_df['Model'], results_df['Test_R2'], color='steelblue')\n",
    "    plt.title(f'Model Comparison - {target_name}')\n",
    "    plt.ylabel('Test R²')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualization 2: Overfitting (Train vs Test R²)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(results_df['Train_R2'], results_df['Test_R2'], s=100)\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Ideal (Train=Test)')\n",
    "    for _, row in results_df.iterrows():\n",
    "        plt.annotate(row['Model'], (row['Train_R2'], row['Test_R2']))\n",
    "    plt.xlabel('Train R²')\n",
    "    plt.ylabel('Test R²')\n",
    "    plt.legend()\n",
    "    plt.title('Overfitting Analysis')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    results_df.to_csv(f'model_results_{target_name}.csv', index=False)\n",
    "    print(f\"\\nResults saved to: model_results_{target_name}.csv\")\n",
    "\n",
    "    return results_df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd30975",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, predictions = compare_models(\n",
    "    X_train, X_test, y_train, y_test, target_name='yield_strength_mpa'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0a5dc",
   "metadata": {},
   "source": [
    "## 6. Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342df807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_results(results_df, target_name='Target'):\n",
    "    \"\"\"\n",
    "    Interpret the best model’s performance using key metrics.\n",
    "    \"\"\"\n",
    "    best = results_df.iloc[0]\n",
    "    print(f\"\\n{'='*80}\\nResults Interpretation - {best['Model']}\\n{'='*80}\")\n",
    "    print(f\"R² (Test): {best['Test_R2']:.4f}\")\n",
    "    print(f\"RMSE: {best['Test_RMSE']:.2f}\")\n",
    "    print(f\"MAE: {best['Test_MAE']:.2f}\")\n",
    "    print(f\"MAPE: {best['Test_MAPE']:.2f}%\")\n",
    "    print(f\"Cross-Validation R²: {best['CV_R2_mean']:.4f} ± {best['CV_R2_std']:.4f}\")\n",
    "    print(f\"Overfitting (Train-Test): {best['Overfitting']:.4f}\")\n",
    "\n",
    "    if best['Overfitting'] > 0.1:\n",
    "        print(\" The model may be overfitting. Try regularization or more data.\")\n",
    "    elif best['Test_MAPE'] < 10:\n",
    "        print(\" Very good prediction accuracy.\")\n",
    "    else:\n",
    "        print(\" Model performance acceptable but could be improved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_results(results_df, 'yield_strength_mpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89040c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "model = get_models()[best_model_name]\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "importances.nlargest(10).plot(kind='barh', figsize=(8,5))\n",
    "plt.title(f\"Top Features - {best_model_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602f3d3",
   "metadata": {},
   "source": [
    " ## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51208f2",
   "metadata": {},
   "source": [
    "### Most appropriate model\n",
    "Among all tested models, **[insert best_model_name]** achieved the best generalization score\n",
    "(R² ≈ high, low RMSE and MAE).\n",
    "\n",
    "### Insights\n",
    "The most influential process parameters include **[variable1]**, **[variable2]**, **[variable3]**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
